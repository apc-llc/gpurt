#include "Check.h"
#include "CLgpu.h"
#if !defined(NDEBUG)
#include "CUDAgpu.h"
#endif
#include "res_embed.h"

#include <cstring>
#include <iostream>
#include <map>
#include <sstream>
#include <vector>

// The maximum number of registers the GPU kernels are expected to have.
// This value is used to calculate the maximum number of active ("persistent") blocks
// the target GPU can physically process in parallel without preemption.
#ifndef NREGS
#define NREGS 32
#endif

using namespace std;

bool CLgpu::initGPU()
{
	if (fatalError != CL_SUCCESS)
		return false;

#if !defined(NDEBUG)
	if (cudaGPU.get())
		return cudaGPU->initGPU();
#endif
	return true;
}

bool CLgpu::isAvailable()
{
	if (!initGPU())
		return false;

	return (ngpus > 0);
}

const string& CLgpu::getPlatformName()
{
	static const string platformName = "OPENCL";
	return platformName;
}

int CLgpu::getBlockSize()
{
	return 128;
}

int CLgpu::getBlockCount()
{
	if (!initGPU())
		return 0;

	return nblocks;
}

int CLgpu::getSMCount()
{
	if (!initGPU())
		return 0;

	return nsms;
}

int CLgpu::getConstMemSize()
{
	if (!initGPU())
		return 0;

	return szcmem;
}

int CLgpu::getSharedMemSizePerSM()
{
	if (!initGPU())
		return 0;

	return szshmem;
}

int CLgpu::getSharedMemSizePerBlock()
{
	if (!initGPU())
		return 0;

	return szshmemPerBlock;
}

void* CLgpu::malloc(size_t size)
{
#define MALLOC_ALIGNMENT 256

	if (!initGPU())
		return NULL;

#if !defined(NDEBUG)
	if (cudaGPU.get())
		return cudaGPU->malloc(size);
#endif

	if (!gmem) return NULL;

	if (ptr + size + MALLOC_ALIGNMENT > gmem + szgmem)
		return NULL;
	
	void* result = ptr;
	ptr += size;
	
	ptrdiff_t alignment = (ptrdiff_t)ptr % MALLOC_ALIGNMENT;
	if (alignment)
		ptr += MALLOC_ALIGNMENT - alignment;
	
	return result;
}

// Reset free memory pointer to the beginning of preallocated buffer.
void CLgpu::mfree()
{
	if (!initGPU())
		return;

#if !defined(NDEBUG)
	if (cudaGPU.get())
	{
		cudaGPU->mfree();
		return;
	}
#endif

	ptr = gmem;
}

// Check whether the specified memory address belongs to GPU memory allocation.
bool CLgpu::isAllocatedOnGPU(const void* ptr)
{
	if (!initGPU())
		return false;

#if !defined(NDEBUG)
	if (cudaGPU.get())
		return cudaGPU->isAllocatedOnGPU(ptr);
#endif
	
	if (!gmem) return false;

	if ((ptr >= gmem) && (ptr <= gmem + szgmem))
		return true;
	
	return false;
}

GPUerror_t CLgpu::memset(void* dst, const int val, size_t size)
{
	if (!initGPU())
		return { (cl_int)CL_DEVICE_NOT_AVAILABLE };

#if !defined(NDEBUG)
	if (cudaGPU.get())
		return cudaGPU->memset(dst, val, size);
#endif

	if (!isAllocatedOnGPU(dst))
		return { (cl_int)CL_INVALID_MEM_OBJECT };

	cl_int clError = CL_SUCCESS;

	CL_ERR_CHECK(clError = clEnqueueFillBuffer(cmdQueue, gmem_cl,
		&val, sizeof(val), (ptrdiff_t)dst - (ptrdiff_t)gmem, size, 0, NULL, NULL));
	
	return { clError };
}

GPUerror_t CLgpu::memcpy(void* dst, const void* src, size_t size, memcpyKind kind)
{
	if (!initGPU())
		return { (cl_int)CL_DEVICE_NOT_AVAILABLE };

#if !defined(NDEBUG)
	if (cudaGPU.get())
		return cudaGPU->memcpy(dst, src, size, kind);
#endif

	// In case the kind is unknown.
	cl_int clError = CL_INVALID_PROPERTY;

	if (kind == memcpyHostToDevice)
	{
		if (!isAllocatedOnGPU(dst))
			return { (cl_int)CL_INVALID_MEM_OBJECT };

		CL_ERR_CHECK(clError = clEnqueueWriteBuffer(cmdQueue, gmem_cl,
			CL_TRUE, (ptrdiff_t)dst - (ptrdiff_t)gmem, size, src, 0, NULL, NULL));
	}
	else if (kind == memcpyDeviceToHost)
	{
		if (!isAllocatedOnGPU(src))
			return { (cl_int)CL_INVALID_MEM_OBJECT };

		CL_ERR_CHECK(clError = clEnqueueReadBuffer(cmdQueue, gmem_cl,
			CL_TRUE, (ptrdiff_t)src - (ptrdiff_t)gmem, size, dst, 0, NULL, NULL));
	}
	
	return { clError };
}

GPUerror_t CLgpu::getLastError()
{
	return { fatalError };
}

GPUerror_t CLgpu::launch(dim3 nblocks, dim3 szblock_, unsigned int szshmem, void* stream,
	const char* name, const std::vector<KernelArgument>& kargs)
{
	if (!initGPU())
		return { (cl_int)CL_DEVICE_NOT_AVAILABLE };

#if !defined(NDEBUG)
	// Launch the PTX code generated by NVIDIA OpenCL compiler as a CUDA kernel. 
	if (cudaGPU.get())
	{
		return cudaGPU->launch(nblocks, szblock_, szshmem, stream,
			name, cl_binaries[name].c_str(), kargs);
	}
#endif

	// Make sure the kernel name is indexed.
	if (cl_kernels.find((string)name) == cl_kernels.end())
		return { CL_INVALID_KERNEL };

	cl_kernel kernel = cl_kernels[name];

	{
		cl_int clError;
	
		int narg = 0;
		for (int e = kargs.size(); narg != e; narg++)
		{
			CL_ERR_CHECK(clError = clSetKernelArg(kernel, narg, kargs[narg].size, kargs[narg].addr));
			if (clError != CL_SUCCESS)
				return { clError };
		}

		// Add two extra artificial arguments: local memory pointer and local memory size.
		// NOTE: Need max(1, szshmem) here, because on zero shared memory size the OpenCL shall throw
		// "CL_INVALID_ARG_SIZE ... if arg_size is zero and the argument is declared with the __local qualifier"
		// https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/clSetKernelArg.html
		CL_ERR_CHECK(clError = clSetKernelArg(kernel, narg++, max(1U, szshmem), NULL));
		if (clError != CL_SUCCESS)
			return { clError };
		CL_ERR_CHECK(clError = clSetKernelArg(kernel, narg++, sizeof(unsigned int), &szshmem));
		if (clError != CL_SUCCESS)
			return { clError };

		cl_event event;
		size_t szblock[] = { szblock_.x, szblock_.y, szblock_.z };
		size_t szproblem[] = { szblock_.x * nblocks.x, szblock_.y * nblocks.y, szblock_.z * nblocks.z };
		CL_ERR_CHECK(clError = clEnqueueNDRangeKernel(
			cmdQueue, kernel, 3, NULL, (size_t*)&szproblem, (size_t*)&szblock, 0, NULL, &event));
		if (clError != CL_SUCCESS)
			return { clError };
	}

	return { fatalError };
}

GPUerror_t CLgpu::synchronize()
{
	if (!initGPU())
		return { (cl_int)CL_DEVICE_NOT_AVAILABLE };

#if !defined(NDEBUG)
	if (cudaGPU.get())
		return cudaGPU->synchronize();
#endif

	cl_int clError = CL_SUCCESS;

	CL_ERR_CHECK(clError = clFinish(cmdQueue));
	
	return { clError };
}

CLgpu::CLgpu() : fatalError(CL_SUCCESS), ngpus(0), gmem(NULL), ptr(NULL)
{
	// Define a vector containing extensions required on available OpenCL platforms.
	vector<string> requiredExtensions(0);
	// requiredExtensions.push_back("cl_khr_fp64");

	cl_int clError;

#define CL_RETURN_ON_ERR(x) do { \
	CL_ERR_CHECK(x); if (clError != CL_SUCCESS) { fatalError = clError; return; } \
} while (0)

	// Initialize platforms.
	cl_uint nplatforms = 0;
	CL_RETURN_ON_ERR(clError = clGetPlatformIDs(0, NULL, &nplatforms));
	
	// There must be at least one platform.
	if (!nplatforms) return;

	vector<cl_platform_id> platforms(nplatforms);
	CL_RETURN_ON_ERR(clError = clGetPlatformIDs(nplatforms, &platforms[0], NULL));
	
	// Select platforms containing capable devices.
	vector<cl_platform_id> platformsCapable;
	for (int i = 0; i < nplatforms; i++)
	{
		// Find devices in the current OpenCL platform.
		unsigned int ndevices;
		CL_RETURN_ON_ERR(clError = clGetDeviceIDs(platforms[i], CL_DEVICE_TYPE_ALL, 0, NULL, &ndevices));

		if (!ndevices) continue;

		vector<cl_device_id> devices(ndevices);
		CL_RETURN_ON_ERR(clError = clGetDeviceIDs(platforms[i], CL_DEVICE_TYPE_ALL, ndevices, &devices[0], NULL));
		
		// Check if there is at least one platform device
		// with the required set of capabilities.
		bool hasCapableDevice = false;
		for (int idevice = 0; idevice < ndevices; idevice++)
		{
			size_t szextensions;
			CL_RETURN_ON_ERR(clError = clGetDeviceInfo(devices[idevice], CL_DEVICE_EXTENSIONS, 0, NULL, &szextensions));
			
			vector<char> cextensions(szextensions + 1);
			CL_RETURN_ON_ERR(clError = clGetDeviceInfo(devices[idevice], CL_DEVICE_EXTENSIONS, szextensions, &cextensions[0],
				NULL));
			cextensions[szextensions] = '\0';
			
			string extensions(&cextensions[0]);
			stringstream ss(extensions);
			string extension;
			vector<string> requiredExtensionsW = requiredExtensions;
			int ncapabilities = 0;
			while (ss >> extension)
			{
				if (extension == "") continue;
			
				for (int j = 0; j < requiredExtensions.size(); j++)
				{
					if (requiredExtensionsW[j] == extension)
					{
						requiredExtensionsW[j] = "";
						ncapabilities++;
						break;
					}
				}
			}
			
			if (ncapabilities == requiredExtensionsW.size())
			{
				hasCapableDevice = true;
				break;
			}
		}
		
		if (hasCapableDevice)
			platformsCapable.push_back(platforms[i]);
	}

	platforms = platformsCapable;
	nplatforms = platformsCapable.size();

	// Display information for platforms containing capable devices.
	cout << nplatforms << " OpenCL platform(s) detected :" << endl;
	for (int i = 0; i < nplatforms; i++)
	{
		size_t szname;
		CL_RETURN_ON_ERR(clError = clGetPlatformInfo(platforms[i], CL_PLATFORM_NAME, 0, NULL, &szname));
		vector<char> name(szname + 1);
		CL_RETURN_ON_ERR(clError = clGetPlatformInfo(platforms[i], CL_PLATFORM_NAME, szname, &name[0], NULL));
		name[szname] = '\0';
		
		size_t szvendor;
		CL_RETURN_ON_ERR(clError = clGetPlatformInfo(platforms[i], CL_PLATFORM_VENDOR, 0, NULL, &szvendor));
		vector<char> vendor(szvendor + 1);
		CL_RETURN_ON_ERR(clError = clGetPlatformInfo(platforms[i], CL_PLATFORM_VENDOR, szvendor, &vendor[0], NULL));
		vendor[szvendor] = '\0';

		cout << "\t" << (i + 1) << ". " << (char*)&name[0] << " / " << (char*)&vendor[0] << endl;
	}
	cout << endl;
	
	int iplatform = 0;
	if (nplatforms > 1)
	{
		iplatform = -1;
	
		const char* cplatform = getenv("OPENCL_PLATFORM");
		if (cplatform)
		{
			if (!(stringstream(cplatform) >> iplatform))
				iplatform = -1;
		}
		
		if (iplatform == -1)
		{
			fprintf(stderr, "More than one OpenCL platform found: "
				"please specify one to use in OPENCL_PLATFORM environment variable\n");
			fprintf(stderr, "Defaulting to platform 1\n");

			iplatform = 1;
		}

		iplatform--;
	}

	// Find devices in the selected OpenCL platform.
	CL_RETURN_ON_ERR(clError = clGetDeviceIDs(platforms[iplatform], CL_DEVICE_TYPE_ALL, 0, NULL, &ngpus));

	if (!ngpus) return;

	vector<cl_device_id> devices(ngpus);
	CL_RETURN_ON_ERR(clError = clGetDeviceIDs(platforms[iplatform], CL_DEVICE_TYPE_ALL, ngpus, &devices[0], NULL));

	if (ngpus > 1)
		cout << nplatforms << " OpenCL platform(s) detected :" << endl;
	
	// Get devices name, vendor and OpenCL version.
	vector<char> name;
	vector<char> vendor;
	vector<char> version;
	for (int i = 0; i < ngpus; i++)
	{
		device = devices[i];
	
		size_t szname;
#ifdef CL_DEVICE_BOARD_NAME_AMD
		if (clGetDeviceInfo(device, CL_DEVICE_BOARD_NAME_AMD, 0, NULL, &szname) != CL_SUCCESS)
#endif
		{
			CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_NAME, 0, NULL, &szname));
			name.resize(szname + 1);
			CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_NAME, szname, &name[0], NULL));
		}
#ifdef CL_DEVICE_BOARD_NAME_AMD
		else
		{
			name.resize(szname + 1);
			CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_BOARD_NAME_AMD, szname, &name[0], NULL));
		}
#endif
		name[szname] = '\0';
		
		size_t szvendor;
		CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_VENDOR, 0, NULL, &szvendor));
		vendor.resize(szvendor + 1);
		CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_VENDOR, szvendor, &vendor[0], NULL));
		vendor[szvendor] = '\0';

		size_t szversion;
		CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_OPENCL_C_VERSION, 0, NULL, &szversion));
		version.resize(szversion + 1);
		CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_OPENCL_C_VERSION, szversion, &version[0], NULL));
		version[szversion] = '\0';

		if (ngpus > 1)
			cout << "\t" << (i + 1) << ". " << (char*)&name[0] << " / " << (char*)&version[0] << endl;
	}
	if (ngpus > 1) cout << endl;

	int idevice = 0;
	if (ngpus > 1)
	{
		idevice = -1;
	
		const char* cdevice = getenv("OPENCL_DEVICE");
		if (cdevice)
		{
			if (!(stringstream(cdevice) >> idevice))
				idevice = -1;
		}
		
		if (idevice == -1)
		{
			fprintf(stderr, "More than one OpenCL device found: "
				"please specify one to use in OPENCL_DEVICE environment variable\n");
			fprintf(stderr, "Defaulting to device 0\n");

			idevice = 1;
		}

		idevice--;
	}

	device = devices[idevice];
	{
		size_t szname;
#ifdef CL_DEVICE_BOARD_NAME_AMD
		if (clGetDeviceInfo(device, CL_DEVICE_BOARD_NAME_AMD, 0, NULL, &szname) != CL_SUCCESS)
#endif
		{
			CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_NAME, 0, NULL, &szname));
			name.resize(szname + 1);
			CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_NAME, szname, &name[0], NULL));
		}
#ifdef CL_DEVICE_BOARD_NAME_AMD
		else
		{
			name.resize(szname + 1);
			CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_BOARD_NAME_AMD, szname, &name[0], NULL));
		}
#endif
		name[szname] = '\0';
		
		size_t szvendor;
		CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_VENDOR, 0, NULL, &szvendor));
		vendor.resize(szvendor + 1);
		CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_VENDOR, szvendor, &vendor[0], NULL));
		vendor[szvendor] = '\0';

		size_t szversion;
		CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_OPENCL_C_VERSION, 0, NULL, &szversion));
		version.resize(szversion + 1);
		CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_OPENCL_C_VERSION, szversion, &version[0], NULL));
		version[szversion] = '\0';
	}

#if !defined(NDEBUG)
	if (!strcmp(&vendor[0], "NVIDIA Corporation"))
	{
		cout << "Enabling debug mode on NVIDIA GPU: ";
		cudaGPU.reset(new CUDAgpu());
		if (!cudaGPU->isAvailable())
		{
			cout << endl;
			fprintf(stderr, "Error: cannot find device for OpenCL debugging on NVIDIA GPU\n");
			return;
		}
	}
#endif

	// Create context.
	context = clCreateContext(NULL, 1, &device, NULL, NULL, &clError);

	// Build programs for all embedded OpenCL sources.
	for (auto& i : res::embed::get_all())
	{
		const string& name = std::get<0>(i);
		const char* source = std::get<1>(i);
		size_t szsource = std::get<2>(i);

		programs.push_back(clCreateProgramWithSource(context, 1, &source, &szsource, &clError));
		CL_RETURN_ON_ERR(clError);
		
		cl_program& program = programs.back();

		string options = "";
		if (!strcmp(&vendor[0], "NVIDIA Corporation"))
		{
			options += "-cl-nv-verbose";

			// Specify the maximum register count equal to NREGS
			{
				stringstream ss;
				ss << " -cl-nv-maxrregcount=";
				ss << NREGS;
				options += ss.str();
			}

#if !defined(NDEBUG)
			options += " -cl-nv-opt-disable";
			options += " -nv-line-info";
			options += " -nv-debug-info";
#else
			options += " -D__FAST_RELAXED_MATH__ -cl-fast-relaxed-math";
#endif
		}

		cl_int clBuildError = clBuildProgram(program, 1, &device, options.c_str(), NULL, NULL);

		// Read the build log.
		size_t szlog = 0;
		CL_RETURN_ON_ERR(clError = clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, 0, NULL, &szlog));
		vector<char> log(szlog + 1);
		CL_RETURN_ON_ERR(clError = clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, szlog, &log[0], NULL));
		log[szlog] = '\0';

		string eol = "";		
		{
			stringstream ss;
			ss << endl;
			eol = ss.str();
		}

#if defined(NDEBUG)			
		if (clBuildError != CL_SUCCESS)
#endif
		{
			if ((string)(char*)&log[0] != eol)
				cout << (char*)&log[0] << endl;
		}

		// Record kernel intermediate representation (binary).
		{		
			cl_int nprograms;
			CL_RETURN_ON_ERR(clGetProgramInfo(program, CL_PROGRAM_NUM_DEVICES, sizeof(cl_int), &nprograms, NULL));
		
			vector<size_t> programSizes(nprograms);
			CL_RETURN_ON_ERR(clGetProgramInfo(program, CL_PROGRAM_BINARY_SIZES, nprograms * sizeof(size_t), &programSizes[0], NULL));
		
			vector<vector<unsigned char> > programBinaries(nprograms);
			vector<unsigned char*> programBinariesPtrs(nprograms);
			for (int i = 0; i < nprograms; i++)
			{
				programBinaries[i].resize(programSizes[i]);
				programBinariesPtrs[i] = &programBinaries[i][0];
			}

			CL_RETURN_ON_ERR(clGetProgramInfo(program, CL_PROGRAM_BINARIES, nprograms * sizeof(unsigned char*),
				&programBinariesPtrs[0], NULL));

			if (nprograms != 1)
			{
				fprintf(stderr, "Expecting exactly one program binary, but got %d\n", nprograms);
				clError = CL_BUILD_PROGRAM_FAILURE;
				CL_RETURN_ON_ERR(clError);
			}
			
			cl_binaries[name] = string((char*)&programBinariesPtrs[0][0], programSizes[0]);
		}

		// Create compiled kernel.
		cl_kernel kernel = clCreateKernel(program, name.c_str(), &clError);
		
		if (clError != CL_SUCCESS)
		{			
			cout << cl_binaries[name] << endl;

			CL_RETURN_ON_ERR(clError);
		}

		clError = clBuildError;
		CL_RETURN_ON_ERR(clError);
				
		cl_kernels[name] = kernel;
		
		cout << "Compiled OpenCL kernel \"" << name << "\"" << endl;
	}

	// Create command queue.
	cmdQueue = clCreateCommandQueue(context, device, CL_QUEUE_PROFILING_ENABLE, &clError);
	CL_ERR_CHECK(clError = clError);

	const int zero = 0;

	cl_mem maxConcurrentBlocks = clCreateBuffer(context, CL_MEM_READ_WRITE, sizeof(int), NULL, &clError);
	CL_RETURN_ON_ERR(clError);
	CL_RETURN_ON_ERR(clError = clEnqueueWriteBuffer(cmdQueue, maxConcurrentBlocks,
		CL_TRUE, 0, sizeof(int), &zero, 0, NULL, NULL));
	
	cl_mem maxConcurrentBlockEvalDone = clCreateBuffer(context, CL_MEM_READ_WRITE, sizeof(int), NULL, &clError);
	CL_RETURN_ON_ERR(clError);
	CL_RETURN_ON_ERR(clError = clEnqueueWriteBuffer(cmdQueue, maxConcurrentBlockEvalDone,
		CL_TRUE, 0, sizeof(int), &zero, 0, NULL, NULL));

	cl_mem bigData = clCreateBuffer(context, CL_MEM_READ_WRITE, 1024 * 1024 * sizeof(float), NULL, &clError);
	CL_RETURN_ON_ERR(clError);
	
	cl_kernel maxConcurrentBlockEval = cl_kernels["maxConcurrentBlockEval"];

	{
		int narg = 0;
		CL_RETURN_ON_ERR(clError = clSetKernelArg(maxConcurrentBlockEval, narg++, sizeof(cl_mem), &maxConcurrentBlocks));
		CL_RETURN_ON_ERR(clError = clSetKernelArg(maxConcurrentBlockEval, narg++, sizeof(cl_mem), &maxConcurrentBlockEvalDone));
		CL_RETURN_ON_ERR(clError = clSetKernelArg(maxConcurrentBlockEval, narg++, sizeof(cl_mem), &bigData));

		cl_event event;
		size_t szblock = getBlockSize();
		size_t szproblem = 1024 * szblock;
		CL_RETURN_ON_ERR(clError = clEnqueueNDRangeKernel(cmdQueue, maxConcurrentBlockEval,
			1, NULL, &szproblem, &szblock, 0, NULL, &event));
		CL_RETURN_ON_ERR(clError = clWaitForEvents(1, &event));
	}

	CL_RETURN_ON_ERR(clError = clEnqueueReadBuffer(cmdQueue, maxConcurrentBlocks, CL_TRUE, 0, sizeof(int), &nblocks, 0, NULL, NULL));

	CL_RETURN_ON_ERR(clError = clReleaseMemObject(maxConcurrentBlocks));
	CL_RETURN_ON_ERR(clError = clReleaseMemObject(maxConcurrentBlockEvalDone));
	CL_RETURN_ON_ERR(clError = clReleaseMemObject(bigData));

	cl_uint nsmsU;
	CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_MAX_COMPUTE_UNITS, sizeof(nsmsU), &nsmsU, NULL));
	nsms = (int)nsmsU;

	cl_ulong szcmemUL;
	CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_MAX_CONSTANT_BUFFER_SIZE, sizeof(szcmemUL), &szcmemUL, 0));
	szcmem = (int)szcmemUL;

	cl_ulong szshmemUL;
	CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_LOCAL_MEM_SIZE, sizeof(szshmemUL), &szshmemUL, 0));
	szshmem = (int)szshmemUL;

	szshmemPerBlock = min(szshmem, (nsms * szshmem) / nblocks);

	cout << "Using GPU \"" << (char*)&name[0] << "\" produced by \"" << (char*)&vendor[0] <<
		"\" with OpenCL version \"" << (char*)&version[0] << "\" : max concurrent blocks = " << nblocks <<
		" : " << szshmemPerBlock << "B of shmem per block" << std::endl;

#if !defined(NDEBUG)
	// Do not preallocate OpenCL memory pool below,
	// if in debug mode on NVIDIA GPU.
	if (cudaGPU.get())
		return;
#endif

	cl_ulong szallocUL;
	CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof(szallocUL), &szallocUL, 0));

	cl_ulong availableUL;
	CL_RETURN_ON_ERR(clError = clGetDeviceInfo(device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof(availableUL), &availableUL, 0));

	// Preallocate 85% of GPU memory to save on costly subsequent allocations.
	szgmem = min((cl_ulong)(0.85 * availableUL), szallocUL);

	gmem_cl = clCreateBuffer(context, CL_MEM_READ_WRITE, szgmem, NULL, &clError);
	CL_RETURN_ON_ERR(clError);

	// Read back the raw pointer behind gmem using a trivial kernel.
	cl_mem ptrDev = clCreateBuffer(context, CL_MEM_READ_WRITE, sizeof(void*), NULL, &clError);
	CL_RETURN_ON_ERR(clError);
	
	cl_kernel getRawPointer = cl_kernels["getRawPointer"];

	{
		int narg = 0;
		CL_RETURN_ON_ERR(clError = clSetKernelArg(getRawPointer, narg++, sizeof(cl_mem), &gmem_cl));
		CL_RETURN_ON_ERR(clError = clSetKernelArg(getRawPointer, narg++, sizeof(cl_mem), &ptrDev));

		cl_event event;
		size_t szblock = 1;
		size_t szproblem = 1;
		CL_RETURN_ON_ERR(clError = clEnqueueNDRangeKernel(cmdQueue, getRawPointer,
			1, NULL, &szproblem, &szblock, 0, NULL, &event));
		CL_RETURN_ON_ERR(clError = clWaitForEvents(1, &event));
	}

	CL_RETURN_ON_ERR(clError = clEnqueueReadBuffer(cmdQueue, ptrDev, CL_TRUE, 0, sizeof(void*), &gmem, 0, NULL, NULL));

	CL_RETURN_ON_ERR(clError = clReleaseMemObject(ptrDev));
	
	ptr = gmem;
}

CLgpu::~CLgpu()
{
	cl_int clError;

#if !defined(NDEBUG)
	// Do not free preallocated OpenCL memory pool below,
	// if in debug mode on NVIDIA GPU.
	if (!cudaGPU.get())
#endif
	{
		CL_RETURN_ON_ERR(clError = clReleaseMemObject(gmem_cl));
	}

	CL_RETURN_ON_ERR(clError = clReleaseCommandQueue(cmdQueue));

	for (map<string, cl_kernel>::iterator i = cl_kernels.begin(), e = cl_kernels.end(); i != e; i++)
		CL_RETURN_ON_ERR(clError = clReleaseKernel(i->second));
	
	for (int i = 0, e = programs.size(); i < e; i++)
		CL_RETURN_ON_ERR(clError = clReleaseProgram(programs[i]));

	CL_RETURN_ON_ERR(clError = clReleaseContext(context));
}

#define CL_CASE_STR(err) case err : { static const string str_##err = #err; return str_##err.c_str(); }

const char* CLgpu::getErrorString(cl_int code)
{
	switch (code)
	{
	CL_CASE_STR(CL_SUCCESS);
	CL_CASE_STR(CL_DEVICE_NOT_FOUND);
	CL_CASE_STR(CL_DEVICE_NOT_AVAILABLE);
	CL_CASE_STR(CL_COMPILER_NOT_AVAILABLE);
	CL_CASE_STR(CL_MEM_OBJECT_ALLOCATION_FAILURE);
	CL_CASE_STR(CL_OUT_OF_RESOURCES);
	CL_CASE_STR(CL_OUT_OF_HOST_MEMORY);
	CL_CASE_STR(CL_PROFILING_INFO_NOT_AVAILABLE);
	CL_CASE_STR(CL_MEM_COPY_OVERLAP);
	CL_CASE_STR(CL_IMAGE_FORMAT_MISMATCH);
	CL_CASE_STR(CL_IMAGE_FORMAT_NOT_SUPPORTED);
	CL_CASE_STR(CL_BUILD_PROGRAM_FAILURE);
	CL_CASE_STR(CL_MAP_FAILURE);
	CL_CASE_STR(CL_MISALIGNED_SUB_BUFFER_OFFSET);
	CL_CASE_STR(CL_EXEC_STATUS_ERROR_FOR_EVENTS_IN_WAIT_LIST);
	CL_CASE_STR(CL_COMPILE_PROGRAM_FAILURE);
	CL_CASE_STR(CL_LINKER_NOT_AVAILABLE);
	CL_CASE_STR(CL_LINK_PROGRAM_FAILURE);
	CL_CASE_STR(CL_DEVICE_PARTITION_FAILED);
	CL_CASE_STR(CL_KERNEL_ARG_INFO_NOT_AVAILABLE);
	CL_CASE_STR(CL_INVALID_VALUE);
	CL_CASE_STR(CL_INVALID_DEVICE_TYPE);
	CL_CASE_STR(CL_INVALID_PLATFORM);
	CL_CASE_STR(CL_INVALID_DEVICE);
	CL_CASE_STR(CL_INVALID_CONTEXT);
	CL_CASE_STR(CL_INVALID_QUEUE_PROPERTIES);
	CL_CASE_STR(CL_INVALID_COMMAND_QUEUE);
	CL_CASE_STR(CL_INVALID_HOST_PTR);
	CL_CASE_STR(CL_INVALID_MEM_OBJECT);
	CL_CASE_STR(CL_INVALID_IMAGE_FORMAT_DESCRIPTOR);
	CL_CASE_STR(CL_INVALID_IMAGE_SIZE);
	CL_CASE_STR(CL_INVALID_SAMPLER);
	CL_CASE_STR(CL_INVALID_BINARY);
	CL_CASE_STR(CL_INVALID_BUILD_OPTIONS);
	CL_CASE_STR(CL_INVALID_PROGRAM);
	CL_CASE_STR(CL_INVALID_PROGRAM_EXECUTABLE);
	CL_CASE_STR(CL_INVALID_KERNEL_NAME);
	CL_CASE_STR(CL_INVALID_KERNEL_DEFINITION);
	CL_CASE_STR(CL_INVALID_KERNEL);
	CL_CASE_STR(CL_INVALID_ARG_INDEX);
	CL_CASE_STR(CL_INVALID_ARG_VALUE);
	CL_CASE_STR(CL_INVALID_ARG_SIZE);
	CL_CASE_STR(CL_INVALID_KERNEL_ARGS);
	CL_CASE_STR(CL_INVALID_WORK_DIMENSION);
	CL_CASE_STR(CL_INVALID_WORK_GROUP_SIZE);
	CL_CASE_STR(CL_INVALID_WORK_ITEM_SIZE);
	CL_CASE_STR(CL_INVALID_GLOBAL_OFFSET);
	CL_CASE_STR(CL_INVALID_EVENT_WAIT_LIST);
	CL_CASE_STR(CL_INVALID_EVENT);
	CL_CASE_STR(CL_INVALID_OPERATION);
	CL_CASE_STR(CL_INVALID_GL_OBJECT);
	CL_CASE_STR(CL_INVALID_BUFFER_SIZE);
	CL_CASE_STR(CL_INVALID_MIP_LEVEL);
	CL_CASE_STR(CL_INVALID_GLOBAL_WORK_SIZE);
	CL_CASE_STR(CL_INVALID_PROPERTY);
	CL_CASE_STR(CL_INVALID_IMAGE_DESCRIPTOR);
	CL_CASE_STR(CL_INVALID_COMPILER_OPTIONS);
	CL_CASE_STR(CL_INVALID_LINKER_OPTIONS);
	CL_CASE_STR(CL_INVALID_DEVICE_PARTITION_COUNT);
	}

	const static string str_CL_UNKNOWN = "Unknown OpenCL error code";
	return str_CL_UNKNOWN.c_str();
}

#if !defined(NDEBUG)
unique_ptr<CUDAgpu> cudaGPU;
#endif

